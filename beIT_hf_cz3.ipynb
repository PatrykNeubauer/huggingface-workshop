{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "beIT - cz4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace Accelerate\n",
        "Accelerate to biblioteka od HuggingFace, będąca prostym API pozwalającym na łatwe przyśpieszenie kodu dzięki m.in. mieszanej precyzji, przetwarzanie na wielu GPU/TPU"
      ],
      "metadata": {
        "id": "zGxMn9BWezhk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bH3AopTew66"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers[sentencepiece]==4.18.0\n",
        "!pip3 install datasets==1.15.1\n",
        "!pip3 install huggingface_hub>=0.1.0,<1.0.0\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wykorzystamy wcześniejszy przykład z porównywaniem pytań na bazie qqp z glue, który rozbudujemy o wykorzystywanie Accelerate.\n",
        "Na początku przeprowadzamy całą inicjalizację i tokenizację w taki sam sposób."
      ],
      "metadata": {
        "id": "v1kIp1BeeywB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer, AdamW, get_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def tokenize(sample):\n",
        "  return tokenizer(sample['question1'], sample['question2'], truncation=True)\n",
        "\n",
        "checkpoint = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "raw_dataset = load_dataset(\"glue\", \"qqp\")\n",
        "\n",
        "raw_dataset['train'] = raw_dataset['train'].shard(num_shards=100, index=0)\n",
        "raw_dataset['test'] = raw_dataset['test'].shard(num_shards=100, index=0)\n",
        "raw_dataset['validation'] = raw_dataset['validation'].shard(num_shards=100, index=0)\n",
        "\n",
        "tokenized_datasets = raw_dataset.map(tokenize)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['question1', 'question2', 'idx'])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "wjHDyXyPgIaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teraz zaczynają się dopiero zmiany związane z Accelerate:\n",
        "Usuwamy wszelkie *.to_device()* (ewentualnie *.cuda()*), jako że tym zajmie się Accelerate.\\\n",
        "W funkcji musimy stworzyć obiekt klasy Accelerator, a następnie przekazać do jego metody prepare() wszystkie obiekty związane z treningiem.\n",
        "Dodatkowo, zamiast *loss.backward()*, używamy *accelerator.backward(loss)*"
      ],
      "metadata": {
        "id": "lEj9mnFMsu1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator() # \n",
        "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
        "  model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
        ") #\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss) # \n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ],
      "metadata": {
        "id": "0qrGIvL-tKQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jednak z powodu specyfiki działania Accelerate, aby zadziałało ono w formie notatnika, całość musimy zamknąć w funkcji, którą następnie uruchomimy przekazując ją do *notebook_launcher*."
      ],
      "metadata": {
        "id": "S1Bo30kUovTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from accelerate import Accelerator\n",
        "from tqdm.auto import tqdm\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer, AdamW, get_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def tokenize(sample):\n",
        "  return tokenizer(sample['question1'], sample['question2'], truncation=True)\n",
        "  \n",
        "def train():\n",
        "  checkpoint = 'bert-base-uncased'\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "  raw_dataset = load_dataset(\"glue\", \"qqp\")\n",
        "\n",
        "  raw_dataset['train'] = raw_dataset['train'].shard(num_shards=100, index=0)\n",
        "  raw_dataset['test'] = raw_dataset['test'].shard(num_shards=100, index=0)\n",
        "  raw_dataset['validation'] = raw_dataset['validation'].shard(num_shards=100, index=0)\n",
        "\n",
        "  tokenized_datasets = raw_dataset.map(tokenize)\n",
        "  tokenized_datasets = tokenized_datasets.remove_columns(['question1', 'question2', 'idx'])\n",
        "  tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "  tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "\n",
        "\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "  train_dataloader = DataLoader(\n",
        "      tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        "  )\n",
        "  eval_dataloader = DataLoader(\n",
        "      tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        "  )\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "  num_epochs = 3\n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  lr_scheduler = get_scheduler(\n",
        "      \"linear\",\n",
        "      optimizer=optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=num_training_steps,\n",
        "  )\n",
        "  \n",
        "  accelerator = Accelerator()\n",
        "  model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
        "  )\n",
        "\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "      for batch in train_dataloader:\n",
        "          outputs = model(**batch)\n",
        "          loss = outputs.loss\n",
        "          accelerator.backward(loss)\n",
        "\n",
        "          optimizer.step()\n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          progress_bar.update(1)"
      ],
      "metadata": {
        "id": "vC9akkA101lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "notebook_launcher(train)"
      ],
      "metadata": {
        "id": "DWFAzeg7nYFk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}