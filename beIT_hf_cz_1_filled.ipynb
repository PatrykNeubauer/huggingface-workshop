{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "beIT - cz.2 - rozwiązania.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wprowadzenie do używania HuggingFace"
      ],
      "metadata": {
        "id": "f3ajihhCDhRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalacja bibliotek HF"
      ],
      "metadata": {
        "id": "oGDqLsD-E3Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers[sentencepiece]==4.18.0\n",
        "!pip3 install datasets==1.15.1\n",
        "!pip3 install huggingface_hub>=0.1.0,<1.0.0"
      ],
      "metadata": {
        "id": "6IjJDmQ9E6CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines"
      ],
      "metadata": {
        "id": "o3-d0hPuQhpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na najwyższym poziomie abstrakcji znajduje się pipeline() - jak wskazuje nazwa jest to gotowy pipeline, pod którym kryje się tokenizer i model, oraz cały związany z tym pre- i postprocessing. "
      ],
      "metadata": {
        "id": "Q38JEzWjEIoz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkG_4_zM0IUl"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"sentiment-analysis\")\n",
        "pipe(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inne gotowe implementacje, które możemy przetestować to m.in.:\n",
        "* feature-extraction - wektoryzacja tekstu\n",
        "* fill-mask - uzupełnianie ukrytych słów (oznaczonych przez \\<mask\\>)\n",
        "* ner (named entity recognition) - rozpoznanie 'bytów' w tekście\n",
        "* question-answering - odpowiadanie na pytania\n",
        "* sentiment-analysis - analiza (klasyfikacja) sentymentu\n",
        "* summarization - podsumowanie tekstu\n",
        "* text-generation - generowanie tekstu\n",
        "* translation - tłumaczenie \n",
        "* zero-shot-classification - klasyfikacja zero-shot\n",
        "\n",
        "---\n",
        "\n",
        "Jeśli przyjrzymy się wynikom wykonanego przez nas wcześniej kodu, pipeline() informuje nas o tym, że skoro nie podaliśmy wprost konkretnego modelu, użyty zostanie pewien z góry ustalony model domyślny. \n",
        "\n",
        "Poniżej ponownie stworzymy pipeline dla analizy sentymentu z tym samym modelem (distilbert-base-uncased-finetuned-sst-2-english), ale tym razem wprost zainicjalizujemy model oraz odpowiadający mu tokenizer.\n",
        "\n",
        "Do inicjalizacji odpowiedniego modelu możemy użyć [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/autoclass_tutorial#autotokenizer) oraz [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification)."
      ],
      "metadata": {
        "id": "dUxSM44DQtmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_name)\n",
        "\n",
        "pipe = pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
        "pipe(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "THSbvscEQrTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Osiągneliśmy te same wyniki co ostatnio, więc już wszystko się zgadza. "
      ],
      "metadata": {
        "id": "mHoDMVGsTAEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gdy spróbujemy użyć po prostu [AutoModel](https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/auto#auto-classes):"
      ],
      "metadata": {
        "id": "N7Kk_0IxXc26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_name)\n",
        "model = AutoModel.from_pretrained(checkpoint_name)\n",
        "\n",
        "pipe = pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
        "pipe(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "F4pEWNPwXhxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dlaczego otrzymaliśmy błąd? Jeśli przyjrzymy się pierwszej linijce, możemy zauważyć ostrzeżenie o tym, że część wag jest ignorowana - a konkretnie wagi klasyfikatora, który interpretuje logits, które są wyjściem transformerów. \n",
        "\n",
        "AutoModel zakłada, że gdy chcemy wczytać model, chodzi nam tylko i wyłącznie o enkodery/dekodery, ignorowane są jakiekolwiek 'głowy'. \n",
        "Aby wczytać także wagi klasyfikatora, musimy być bardziej precyzyjni i użyć odpowiedniego AutoModel dla danego zadania."
      ],
      "metadata": {
        "id": "us-51lN9S3_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "W formie prostego zadania, spróbuj na podstawie tego co zostało już przedstawione, stworzyć pipeline do tłumaczenia tekstu z polskiego na angielski. \n",
        "\n",
        "Podpowiedź - wyszukaj odpowiedni checkpoint na [HuggingFace](https://huggingface.co/models) oraz klasę [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) dla tego typu zadań (seq2seq)."
      ],
      "metadata": {
        "id": "kkVSGEpZW2X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O-6PDNIMurU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rozwiązanie:\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "checkpoint_name = 'Helsinki-NLP/opus-mt-pl-en'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_name)\n",
        "\n",
        "pipe = pipeline(\"translation\", tokenizer=tokenizer, model=model)\n",
        "pipe(\n",
        "    [\n",
        "        \"Losowy tekst po polsku\",\n",
        "        \"Drugi tekst po polsku\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "e9o4f57-xnMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- \n",
        "\n",
        "Teraz jest dobry moment, żeby dokładniej się przyjrzeć temu, co się dzieje między wejściem tekstu, a wyjściem modelu.\n",
        "\n",
        "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
        "\n",
        "Tak przedstawia się ogólny zarys, działanie modelu powinniśmy rozumieć, czas natomiast przyjrzeć się temu jak zamieniamy tekst w wejście do modelu."
      ],
      "metadata": {
        "id": "HRsNwwSfxx3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizacja\n",
        "Tokenizacja odpowiada za zmianę tekstu (który mogą zrozumieć ludzie) w liczby (które może zrozumieć komputer). \n",
        "\n",
        "Wyróżniane są głównie 3 sposoby tokenizacji:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Na poziomie słów**\\\n",
        "Działa dobrze, ale efektem ubocznym są ogromne słowniki - każde słowo musi mieć swoje własne ID, jest to jeszcze bardziej problematyczne w polskim, gdzie każda odmiana pewnego słowa byłaby dodatkowym wejściem (możnaby użyć lematyzacji, ale to także ma swoje wady). Dodatkowo każde słowo z poza słownika, musi być zastąpione tokenem \\<unk\\>.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Na poziomie znaków**\\\n",
        "Eliminuje problem z rozmiarem słownika, znacznie zredukuje także częstość występowania \\<unk\\>. Tutaj natomiast problemem jest to, że znacznie rośnie ilość tokenów, które musi przetworzyć model, dodatkowo pojedyncze tokeny mają znacznie mniejsze \"znaczenie\", jako że reprezentują znaki zamiast słów.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Na poziomie podsłów**\\\n",
        "Rozwiązanie pośrednie pomiędzy podziałem na słowa, a na znaki, mające łączyć zalety obu sposobów - polega na podziale słów na pomniejsze fragmenty. Obecnie najczęściej używany sposób, istnieje kilka sposobów podziału na podsłowa. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Przykład jak podzielone zostało by \"Zdanie do tokenizacji\" tymi 3 sposobami:\\\n",
        "![](https://drive.google.com/uc?id=1mn4BiH9IkrfdvMl-7n1h7J4ITj3rcIds)\n",
        "\n"
      ],
      "metadata": {
        "id": "lGJSdDoCJyWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Przetwarzanie języka naturalnego to interdyscyplinarna dziedzina łącząca zagadnienia sztucznej inteligencji i językoznawstwa\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "\n",
        "print(\"Tokenizacja na poziomie słow: \", text.split())\n",
        "print(\"Tokenizacja na poziomie znaków: \", [char for char in text.replace(' ', '')])\n",
        "print(\"Tokenizacja na poziomie podsłów (konkretnie byte-pair encoding): \", tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "N4MR7rXlOQ3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gdy tekst jest już podzielony na tokeny, można przy pomocy słownika tokenizera zmienić je na ID."
      ],
      "metadata": {
        "id": "vcdrgnvV-xxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "-ik9iizJ-w7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab"
      ],
      "metadata": {
        "id": "lhH-ydfdXB9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te ID, w teorii mogą już być wysłane do modelu po owinięciu ich w tensor odpowiedniego frameworku (w naszym przypadku PyTorch).\n",
        "\n",
        "Po wcześniejszych eksperymentach, jeszcze raz przypisujemy model i tokenizator - w przypadku pracy z wieloma modelami należy pamiętać żeby **zawsze używać tokenizatora odpowiadającego modelowi!**\n",
        "\n",
        "Między niektórymi modelami teoretycznie może się zdarzyć, że tokenizatory będą kompatybilne, ale w większości przypadków przez różnice w słowniku napotkamy wyjątek, lub przez to że pod jednym ID tokenizator i model mają zupełnie inny token, zwracane przez model wyjście będzie bezsensu."
      ],
      "metadata": {
        "id": "6DLFaxPIJ0G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "checkpoint_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_name)\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor([ids])\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "id": "geSTzDxpJt4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dla pewności, możemy także z powrotem zmienić ID na tekst, jest to też przydatne, gdy nasz model będzie miał generować tekst."
      ],
      "metadata": {
        "id": "uOixsHTI_tD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "id": "sTqIs5G3_5eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jednak, gdy tym sposobem jak wcześniej spróbujemy przekazać do modelu na raz kilka zdań, napotkamy błąd - w praktyce są jeszcze dwie ważne rzeczy, którymi zajmują się tokenizatory:\n",
        "- truncation - przycięcie zbyt długich wektorów tokenów, do maksymalnej akceptowanej przez model\n",
        "- padding - przy wielu tekstach (batch), wyrównujemy wszystkie tensory z tokenami do nadłuższego w danym batchu, poprzez dodawanie specjalnego tokenu (\\<pad\\>, 0, ...) do krótszych tensorów\n",
        "\n",
        "Ze względu na atencję w transformerach, aby w trakcie liczenia atencji nie były brane pod uwagę tokeny paddingu, tokenizatory zwracają także *attention_mask*, określające czy dany token powinien być wliczany przy atencji.\n",
        "\n",
        "Teraz zrozumiałe powinny być wartości zwracane przez tokenizer:"
      ],
      "metadata": {
        "id": "On7WAMYYNGn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        "\n",
        "tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "LihIEeh2sMNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dodatkowo, przy niektórych modelach wstawiane mogą być dodatkowe tokeny, przykładowo BERT przy zadaniach klasyfikacji, oczekuje na początku wejścia specjalny token [CLS] oraz na koniec każdego tekstu [SEP], pozwalający mu lepiej zrozumieć koniec jednego wejścia i początek drugiego.  "
      ],
      "metadata": {
        "id": "DgYyXgNLvxe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_input = tokenizer(\"Do We Have Any Mathematical Proof That Pi Is Infinite?\",\" Does pi go on indefinitely?\", return_tensors='pt')\n",
        "tokenizer.decode(encoded_input[\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "OxvZaBiSvpYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "Gdy doszliśmy już do tego co otrzymuje model, możemy przyjrzeć się dokładniej co znajduje się na jego wyjściu"
      ],
      "metadata": {
        "id": "uCs8qGQr-8r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ],
      "metadata": {
        "id": "g6zn2oOU_PWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(encoded_input['input_ids'])"
      ],
      "metadata": {
        "id": "FB4DCADrBNBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "EPzjTd-dBfQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(encoded_input['input_ids'])\n",
        "output['last_hidden_state'].shape"
      ],
      "metadata": {
        "id": "64PQ8FjRBj00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trening modeli\n",
        "Wiedząc już dokładniej co robi tokenizer, możemy przejść do trenowania modeli - przeprowadzimy prosty fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "Dalszy przykład będzie na podstawie zbioru danych [glue](https://huggingface.co/datasets/glue) - jest to jeden z popularniejszych zbiorów NLP, z podzbiorami do wielu różnych zadań. Skupimy się na podzbiorze *qqp* - jest to zbiór par pytań z serwisu Quora, wraz z oznaczeniem czy dane pytania są semantycznie takie same.\n",
        "Zatem klasyfikator wytrenowany na tym zbiorze, będzie w stanie stwierdzić czy podane mu dwa pytania, pytają się o to samo.\n",
        "\n",
        "Jako model użyjemy do tego [BERTa](https://huggingface.co/bert-base-uncased).\n",
        "\n",
        "---\n",
        "\n",
        "Tu kolejną bardzo przydatną funkcjonalnością HuggingFace, jest ich biblioteka *datasets*, która pozwala w prosty i szybki sposób pobrać i przygotować dane do treningu."
      ],
      "metadata": {
        "id": "FbLn0br6Nm-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zbiór danych i tokenizacja\n"
      ],
      "metadata": {
        "id": "S7zbU9Sjg1eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_dataset = load_dataset(\"glue\", \"qqp\")"
      ],
      "metadata": {
        "id": "UnWG_qTJOudT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFace zaimplementowało *datasets*, w taki sposób że w pamięci RAM trzymane są tylko te przypadki, które potrzebujemy, a reszta jest trzymana na dysku. Zwalnia nas to z własnoręcznego zarządzania pamięcią, która często bywa konieczna, przy tym jakie rozmiary mają niektóre zbiory w NLP."
      ],
      "metadata": {
        "id": "bqLGThxVZ4aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset"
      ],
      "metadata": {
        "id": "eiunCrOZRtid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na potrzeby warsztatu weźmiemy pod uwagę tylko 10% zbioru"
      ],
      "metadata": {
        "id": "nB6Vp89qv5Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset['train'] = raw_dataset['train'].shard(num_shards=100, index=0)\n",
        "raw_dataset['test'] = raw_dataset['test'].shard(num_shards=100, index=0)\n",
        "raw_dataset['validation'] = raw_dataset['validation'].shard(num_shards=100, index=0)"
      ],
      "metadata": {
        "id": "-w9PA5YQv4Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jak widzimy gotowy jest już podział na zbiór treningowy, testowy i walidacyjny. \n",
        "\n",
        "Możemy jeszcze się przyjrzeć jak wygląda pojedynczy przypadek:"
      ],
      "metadata": {
        "id": "J_1NgSkURruu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset['train'].features\n"
      ],
      "metadata": {
        "id": "o7evuci9Y7zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset['train'][0]"
      ],
      "metadata": {
        "id": "FHP4I_qYSA_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dane te trzeba jeszcze przygotować, zanim będziemy mogli użyć je do treningu, gdy spojrzymy na kartę modelu, możemy zauważyć, że jako wejście powinniśmy dawać do modelu:\\\n",
        "[CLS] \\<question1\\> [SEP] \\<question2\\>\n",
        "\n",
        "Jak było widoczne na przykładzie wcześniej, jest to już coś co obsługuje tokenizer modelu:\n"
      ],
      "metadata": {
        "id": "jJHZmm8IZBpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "VV6tw-TKaz2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = raw_dataset['train'][0]\n",
        "tokenizer.decode(tokenizer(sample['question1'], sample['question2'])['input_ids'])"
      ],
      "metadata": {
        "id": "Ueqc0pK2bFiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Musimy zatem w ten sposób przetworzyć cały zbiór, aby dalej korzystać z ułatwień *datasets*, zamiast iterować i tworzyć własny zbiór z zwracanych przez tokenizer dictów, możemy użyć [map()](https://huggingface.co/docs/datasets/v2.1.0/en/package_reference/main_classes#datasets.Dataset.map) po zdefiniowany naszej tokenizacji w formie funkcji. "
      ],
      "metadata": {
        "id": "2VC2LyKGWeXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sample):\n",
        "  return tokenizer(sample['question1'], sample['question2'], truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_dataset.map(tokenize)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "udLZigrfXGMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode( tokenized_datasets['train'][0]['input_ids'] )"
      ],
      "metadata": {
        "id": "Tbflyu5_dcgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding"
      ],
      "metadata": {
        "id": "Gzzgla6UhBRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Została jeszcze jedna kwestia - padding. \n",
        "Możemy dodać padding bezpośrednio do zbioru, razem z tokenizacją w ten sposób:\n",
        "\n",
        "```\n",
        "tokenized_datasets = raw_dataset.map(tokenize, padding=True)\n",
        "```\n",
        "Rozwiązanie tego w taki sposób ma jednak jeden problem - wszystkie przykłady w zbiorze będą miały dodany padding, tak aby były równe najdłuższemu przykładowi, co w zależności od rozłożenia naszych danych, może mieć znanczny wpływ na prędkość treningu. \n",
        "\n",
        "Lepszym od tego sposobem jest dynamic padding - jest to dodawanie paddingu indywidualnie do każdego batcha, dzięki czemu przykłady nie są długości najdłuższego przypadku w całym zbiorze, a jedynie w konkretnym batchu.\n",
        "\n",
        "--- \n",
        "\n",
        "Funkcja składająca przypadki w batch nazywa się collate, która jest przekazywana do DataLoader'a. \n",
        "Domyślnie funkcja ta zajmuje się jedynie zamianą przypadków w tensory oraz ich konkatenacją, ale zamiast pisania jej ręcznie, możemy użyć [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorWithPadding) z biblioteki *transformers*."
      ],
      "metadata": {
        "id": "lY-DBTy7eZ5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "DGLDcvTsjWIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Najpierw tworzymy obiekt przechowujący argumenty treningowe"
      ],
      "metadata": {
        "id": "CZHAUnHN_GNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"bert-finetuned-qqp\")"
      ],
      "metadata": {
        "id": "YO6mnGYzj1f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args"
      ],
      "metadata": {
        "id": "t8TFfNYp_Orz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjalizacja modelu do klasyfikacji."
      ],
      "metadata": {
        "id": "_Nr8-ajEAniX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "BZFtiD48j4R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trening przy pomocy klasy Trainer"
      ],
      "metadata": {
        "id": "4bLTq32khHvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poniżej tworzymy obiekt klasy Trainer. Zajmuje on się m.in. włączaniem i wyłączaniem gradientów, używaniem Data Loader'ów dla zbiorów treningowych/testowych/walidacyjnych, przenoszeniem odpowiednich danych z CPU na GPU i z powrotem, logowaniem np. do tensorboarda itd., ma też dodatkowe funkcjonalności jak szukanie odpowiedniego learning rate.\n",
        "Eliminuje on więc potrzebę pisania kodu treningu, który w zasadzie w większości przypadków wygląda tak samo."
      ],
      "metadata": {
        "id": "5Es6ml1BBKvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "IwrZv-Qnj87S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9dVW8G5ZkAXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ewaluacja:"
      ],
      "metadata": {
        "id": "1DehJWqLPdm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "metric = load_metric(\"glue\", \"qqp\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ],
      "metadata": {
        "id": "09Q7o2BQPeu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wynik będzie się u każdego różnić, ale model powinnien mieć około 81% celności, co jak na rozmiar użytego zbioru danych i czas treningu, jest nienajgorszy.\n",
        "\n",
        "Możemy spróbować użyć modelu:"
      ],
      "metadata": {
        "id": "1DfNKooMQfvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = raw_dataset['validation'][0]\n",
        "question_1 = sample['question1']\n",
        "question_2 = sample['question2']\n",
        "\n",
        "print(question_1, '\\n', question_2)\n",
        "\n",
        "preds = trainer.predict([tokenizer(question_1, question_2)]).predictions[0]\n",
        "\n",
        "if preds[0] > preds[1]:\n",
        "  print(\"Model predicted that these questions are different.\")\n",
        "else:\n",
        "  print(\"Model predicted that these questions are about the same thing.\")"
      ],
      "metadata": {
        "id": "IJKawkkPQ1BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podsumowanie całego dotychczasowego kodu do fine-tuningu:\n",
        "```\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "def tokenize(sample):\n",
        "  return tokenizer(sample['question1'], sample['question2'], truncation=True)\n",
        "\n",
        "checkpoint = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "raw_dataset = load_dataset(\"glue\", \"qqp\")\n",
        "tokenized_datasets = raw_dataset.map(tokenize)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\"bert-finetuned-qqp\")\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "```"
      ],
      "metadata": {
        "id": "H0KxYQBYEaDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ręczny trening\n",
        "Jeśli jednak nie chcemy wykorzystywać klasy Trainer, bez problemu można samemu przeprowadzić trening, co da nam większią kontrolę.\n",
        "Żeby się nie pogubić, przeprowadzimy całą inicjalizację od nowa.\n"
      ],
      "metadata": {
        "id": "_KLZWKHDhPos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "def tokenize(sample):\n",
        "  return tokenizer(sample['question1'], sample['question2'], truncation=True)\n",
        "\n",
        "checkpoint = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "raw_dataset = load_dataset(\"glue\", \"qqp\")\n",
        "raw_dataset['train'] = raw_dataset['train'].shard(num_shards=50, index=0)\n",
        "raw_dataset['test'] = raw_dataset['test'].shard(num_shards=50, index=0)\n",
        "raw_dataset['validation'] = raw_dataset['validation'].shard(num_shards=10, index=0)\n",
        "tokenized_datasets = raw_dataset.map(tokenize)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "gwTcYoRl0Mh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "VWrbeDPo836x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Najważniejsze rzeczy, które wykonywał Trainer, którymi trzeba się zająć to:\n",
        "* zostawić w Dataset tylko te kolumny, których spodziewa się model (w tym wypadku trzeba usunąć 'question1', 'question2', 'idx', zmienić 'label' na 'labels')\n",
        "* stworzyć DataLoadery, zwracające tensory\n",
        "* zainicjalizować optimizer i  learning rate scheduler.\n",
        "* przenoszenie zmiennych z CPU na GPU i wzajemnie\n",
        "* pętla treningowa i ewaluacji\n"
      ],
      "metadata": {
        "id": "hf3YPYWS8wa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets[\"train\"].column_names"
      ],
      "metadata": {
        "id": "ruM7Wj2m-nIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns(['question1', 'question2', 'idx'])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
      ],
      "metadata": {
        "id": "aP5eHtrU-twq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przygotowanie DataLoader'ów"
      ],
      "metadata": {
        "id": "KQKVfwo9_IR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "Iorb1fhC_Kos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Możemy od razu się upewnić że wszystko działa w poniższy sposób:"
      ],
      "metadata": {
        "id": "vx98upurDs_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    break\n",
        "print({k: v.shape for k, v in batch.items()})   # Tutaj przy okazji można zauważyć, że rozmiar tensorów będzie różny za każdym razem - jest to efekt dynamic padding\n",
        "outputs = model(**batch)\n",
        "print(outputs.loss, outputs.logits.shape)"
      ],
      "metadata": {
        "id": "dT556jUA_Rcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "kIcdqJMBEFe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "print(num_training_steps)"
      ],
      "metadata": {
        "id": "kyX3NP1UEIIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Używanie CUDA jeśli jest dostępna"
      ],
      "metadata": {
        "id": "G4uLz8lfEnl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "rcPcziYTEWlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W tym momencie, tak wyglądałaby pętla treningu:"
      ],
      "metadata": {
        "id": "z0m2nkfFEirN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ],
      "metadata": {
        "id": "Z0E_6vkuEkDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tak ewaluacji:"
      ],
      "metadata": {
        "id": "Lwg5HqlME5FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"glue\", \"mrpc\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ],
      "metadata": {
        "id": "hdK8KBj3E6Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Udostępnianie modeli\n",
        "Możemy też bez problemu udostępniać swoje modele na HuggingFace.\n",
        "\n",
        "Jeśli zamierzamy wytrenować model przy pomocy Trainer, wystarczy dodatkowo przekazać argument *push_to_hub=True* do TrainingArguments, aby model po zakończonym treningu został spushowany do naszego repozytorium.\n",
        "\n",
        "W innym wypadku, aby umieścić model i jego tokenizer w repozytorium, wystarczy zrobić to:\n",
        "```\n",
        "model.push_to_hub(\"model-name\")\n",
        "tokenizer.push_to_hub(\"model-name\")\n",
        "```\n",
        "\n",
        "Przed tym trzeba się jeszcze zalogować do HuggingFace, z poziomu notatnika można to zrobić w ten sposób:\n",
        "```\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "```"
      ],
      "metadata": {
        "id": "671oX5DDHVhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie"
      ],
      "metadata": {
        "id": "ylWCDElIoQ1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na podstawie powyższego przykładu, spróbuj wykonać fine-tuning dla jakiegoś polskiego zadania. \n",
        "Podobnym przykładem byłby fine-tuning [HerBERTa](https://huggingface.co/allegro/herbert-base-cased) na zbiorze [PSC](https://huggingface.co/datasets/psc) - otrzymalibyśmy klasyfikator oparty na BERT, który sprawdzałby czy jeden z podanych tekstów jest podsumowaniem drugiego.\n",
        "\n",
        "Pamiętaj, że należy zwrócić uwagę czy checkpoint modelu, który bierzemy był wytrenowany albo na polskim zbiorze, lub przynajmniej wielojęzycznym, zawierającym polski."
      ],
      "metadata": {
        "id": "YWZZo3fGT_ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A6Tj7yQlYEka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rozwiązanie\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "def tokenize(sample):\n",
        "  return tokenizer(sample['extract_text'], sample['summary_text'], truncation=True)\n",
        "\n",
        "checkpoint = 'allegro/herbert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "raw_dataset = load_dataset(\"psc\", \"default\")\n",
        "\n",
        "raw_dataset['train'] = raw_dataset['train'].shard(num_shards=10, index=0)\n",
        "raw_dataset['test'] = raw_dataset['test'].shard(num_shards=10, index=0)\n",
        "\n",
        "tokenized_datasets = raw_dataset.map(tokenize)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\"test-trainer\",per_device_eval_batch_size=4, per_device_train_batch_size=4)\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Ga6AsSotYFBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = raw_dataset['test'][9]\n",
        "question_1 = sample['extract_text']\n",
        "question_2 = sample['summary_text']\n",
        "label = sample['label']\n",
        "\n",
        "print(question_1, '\\n', question_2, '\\n', label)\n",
        "print(tokenizer.decode(tokenizer(question_1, question_2)['input_ids']))\n",
        "preds = trainer.predict([tokenizer(question_1, question_2)]).predictions[0]\n",
        "\n",
        "if preds[0] > preds[1]:\n",
        "  print(\"Model predicted that these are different.\")\n",
        "else:\n",
        "  print(\"Model predicted that these are about the same thing.\")"
      ],
      "metadata": {
        "id": "WrHcnNxvkPwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}